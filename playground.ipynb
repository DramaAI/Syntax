{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from src.backend.module import nn, modules, loss as loss_module\n",
    "from src.backend.utils import load_base_model, load_tokenizer\n",
    "from torch.optim import Adam\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2023, 2003, 2019, 2742, 102]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"This is an example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[0.6899, 0.3432, 0.7290, 0.2587],\n",
      "        [0.6177, 0.8357, 0.8369, 0.7507],\n",
      "        [0.0174, 0.3885, 0.4299, 0.2320]])\n",
      "tensor([[1, 0, 0, 2],\n",
      "        [1, 2, 2, 1],\n",
      "        [2, 1, 1, 0]])\n",
      "\n",
      "Indices replacements0.5:\n",
      "tensor([[0, 0],\n",
      "        [0, 3],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 2],\n",
      "        [1, 3],\n",
      "        [2, 0],\n",
      "        [2, 1],\n",
      "        [2, 2]])\n",
      "\n",
      "Argmax index of values at those indices:\n",
      "tensor([0, 1, 0, 1, 1, 0, 2, 0, 1])\n",
      "\n",
      "Argmax index of values at those indices:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [9] cannot be broadcast to indexing result of shape [9, 2, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#Y131sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(argmax_index)  \u001b[39m# .item() to get the value as a Python scalar\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#Y131sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mArgmax index of values at those indices:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#Y131sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m tensor[result_indices] \u001b[39m=\u001b[39m argmax_index\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#Y131sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(tensor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [9] cannot be broadcast to indexing result of shape [9, 2, 4]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_indexes_greater_than_threshold(tensor, threshold):\n",
    "    # Create a boolean mask where each element is True if it's greater than the threshold\n",
    "    mask = tensor > threshold\n",
    "    \n",
    "    # Use torch.nonzero to get the indices where the mask is True\n",
    "    indices = torch.nonzero(mask)\n",
    "    \n",
    "    # The result will be a tensor of indices\n",
    "    return indices\n",
    "\n",
    "def get_argmax_of_indices(tensor, indices):\n",
    "    # Use torch.argmax to get the index of the maximum value along a specified dimension\n",
    "    # In this case, we want to find the argmax along the first dimension (batch dimension)\n",
    "    # print(torch.argmax tensor[indices[:, 0], indices[:, 1], ...])\n",
    "    return tensor[indices[:, 0], indices[:, 1]].argmax(axis=-1)\n",
    "\n",
    "# Example usage:\n",
    "batch_size = 3\n",
    "context_size = 4\n",
    "threshold = 0.5\n",
    "\n",
    "# Create a random tensor for demonstration\n",
    "replacement = torch.rand((batch_size, context_size))\n",
    "tensor = torch.randint(0, 3,  size=(batch_size, context_size), dtype=torch.int64)\n",
    "output = torch.rand((batch_size, context_size, 3))\n",
    "\n",
    "# Get the indices where values are greater than the threshold\n",
    "result_indices = get_indexes_greater_than_threshold(tensor.float(), threshold)\n",
    "\n",
    "# Get the argmax of the values at those indices\n",
    "argmax_index = get_argmax_of_indices(output, result_indices)\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(replacement)\n",
    "print(tensor)\n",
    "print(\"\\nIndices replacements{}:\".format(threshold))\n",
    "print(result_indices)\n",
    "print(\"\\nArgmax index of values at those indices:\")\n",
    "print(argmax_index)  # .item() to get the value as a Python scalar\n",
    "print(\"\\nArgmax index of values at those indices:\")\n",
    "tensor[result_indices[:, 0], result_indices[:, 1]] = argmax_index\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def ASSERTION_DATAFRAME_COLUMN(columns):\n",
    "    columns = tuple(columns)\n",
    "    DATASET_COLUMNS_WITH_ID = (\"ID\", \"Input\", \"Output\")\n",
    "    DATASET_COLUMNS = (\"Input\", \"Output\")\n",
    "    return DATASET_COLUMNS == columns or DATASET_COLUMNS_WITH_ID == columns\n",
    "\n",
    "\n",
    "def preprocessed(df, tokenizer, vocab=30522, context_length=512, flag=\"~_~\"):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    assert isinstance(df, pd.DataFrame), \"pre-process precondition needs to be in a DataFrame\"\n",
    "    assert ASSERTION_DATAFRAME_COLUMN(df.columns)\n",
    "\n",
    "    # parse string to convert into a list\n",
    "    df[\"Output\"] = df[\"Output\"].apply(lambda x : x.split(','))\n",
    "\n",
    "    # regular expression that obtains flag and associated word\n",
    "    pattern = rf\"{flag}(\\w+)\"\n",
    "\n",
    "    # if ID exist within the column then remove it\n",
    "    if \"ID\" in df.columns:\n",
    "        df.drop(\"ID\", axis=1, inplace=True)\n",
    "\n",
    "    inputs, outputs = [], []\n",
    "    for i, sentence in enumerate(df[\"Input\"]):\n",
    "        replace = re.findall(pattern, sentence) # find all replaced tokens\n",
    "        replacement : list = df[\"Output\"][i]    # index the associated replacement list\n",
    "        \n",
    "        # replace tokens should equal replacement\n",
    "        if len(replacement) != len(replace):\n",
    "            continue\n",
    "\n",
    "        # construct the replacement dictionary\n",
    "        replacement_dict = {replace : replacement[i] for i, replace in enumerate(replace)}   \n",
    "\n",
    "        # TODO Not ideal in initializing each iteration \n",
    "        def replaced(match):\n",
    "            word = match.group(1)\n",
    "            return replacement_dict.get(word, match.group(0))\n",
    "\n",
    "        output_text = re.sub(pattern, replaced, sentence)\n",
    "        input_text = sentence.replace(flag, \"\")\n",
    "        inputs.append(input_text)\n",
    "        outputs.append(output_text)\n",
    "\n",
    "\n",
    "    input_tokens = tokenizer(inputs, padding=True, max_length=context_length, truncation=True, return_tensors=\"pt\")\n",
    "    # TODO: change to make \n",
    "    output_tokens = tokenizer(outputs, padding=True, max_length=context_length, truncation=True, return_tensors=\"pt\")\n",
    "    mask = ((input_tokens[\"input_ids\"] - output_tokens[\"input_ids\"]) != 0).to(torch.long)\n",
    "    prob = torch.nn.functional.one_hot(output_tokens[\"input_ids\"], num_classes=vocab)\n",
    "    \n",
    "    return input_tokens, output_tokens, mask, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/syntax/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "syntax = nn.SyntaxBert.load_local_weights(nn.BertConfig)\n",
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Labrador Retriever, or simply Labrador, is...</td>\n",
       "      <td>developed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>An angel is a ~_~celestial being in various tr...</td>\n",
       "      <td>supernatural,religions,benevolent,religions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Attachment theory is a psychological, evolutio...</td>\n",
       "      <td>emotional,interactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Gulag was the government ~_~administration...</td>\n",
       "      <td>agency,convicts,convicts,convicts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>TreeHugger is a sustainability website that re...</td>\n",
       "      <td>boasts,bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Mr. Clean (or Mr. Proper) is a brand name and ...</td>\n",
       "      <td>conceived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Foreign Affairs is an American journal of inte...</td>\n",
       "      <td>policy,policy,magazine,policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Power Rangers is an American entertainment and...</td>\n",
       "      <td>series,series,developed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Greenland is a North American autonomous ~_~co...</td>\n",
       "      <td>territory,residents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Amazing Grace' is a Christian hymn published i...</td>\n",
       "      <td>song,song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>The Taj Mahal is an ivory-white marble mausole...</td>\n",
       "      <td>tomb,tomb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>The Eiffel Tower (French: Tour Eiffel) is a wr...</td>\n",
       "      <td>constructed,monument,monument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>The design of the Eiffel Tower is attributed t...</td>\n",
       "      <td>envisioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>Fyodor Mikhailovich Dostoevsky was a Russian ~...</td>\n",
       "      <td>novelist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>Many of his works are considered highly influe...</td>\n",
       "      <td>writings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>Bose Corporation is an American manufacturing ...</td>\n",
       "      <td>develop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>The company was ~_~established in Massachusett...</td>\n",
       "      <td>founded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>Bose Corporation is an American manufacturing ...</td>\n",
       "      <td>corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>Altman invests in technology ~_~companies and ...</td>\n",
       "      <td>startups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>During the depositor run on Silicon Valley Ban...</td>\n",
       "      <td>startups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22</td>\n",
       "      <td>The Lord of the Rings is an epic high-fantasy ...</td>\n",
       "      <td>novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>Tolkien's ~_~story, after an initially mixed r...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>Ontario is one of the thirteen provinces and t...</td>\n",
       "      <td>populous,populous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>The Iliad is one of two major ancient Greek ep...</td>\n",
       "      <td>literature,celebrated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26</td>\n",
       "      <td>Pink Floyd are an English rock band formed in ...</td>\n",
       "      <td>groups,successful,produced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27</td>\n",
       "      <td>The Border Collie is a British breed of herdin...</td>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28</td>\n",
       "      <td>The border collie is descended from landrace c...</td>\n",
       "      <td>breed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>South Africa, officially the Republic of South...</td>\n",
       "      <td>nation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>Coldplay are a British rock band formed in Lon...</td>\n",
       "      <td>received,group,repertoire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                              Input  \\\n",
       "0    1  The Labrador Retriever, or simply Labrador, is...   \n",
       "1    2  An angel is a ~_~celestial being in various tr...   \n",
       "2    3  Attachment theory is a psychological, evolutio...   \n",
       "3    4  The Gulag was the government ~_~administration...   \n",
       "4    5  TreeHugger is a sustainability website that re...   \n",
       "5    6  Mr. Clean (or Mr. Proper) is a brand name and ...   \n",
       "6    7  Foreign Affairs is an American journal of inte...   \n",
       "7    8  Power Rangers is an American entertainment and...   \n",
       "8    9  Greenland is a North American autonomous ~_~co...   \n",
       "9   10  Amazing Grace' is a Christian hymn published i...   \n",
       "10  11  The Taj Mahal is an ivory-white marble mausole...   \n",
       "11  13  The Eiffel Tower (French: Tour Eiffel) is a wr...   \n",
       "12  14  The design of the Eiffel Tower is attributed t...   \n",
       "13  15  Fyodor Mikhailovich Dostoevsky was a Russian ~...   \n",
       "14  16  Many of his works are considered highly influe...   \n",
       "15  17  Bose Corporation is an American manufacturing ...   \n",
       "16  18  The company was ~_~established in Massachusett...   \n",
       "17  19  Bose Corporation is an American manufacturing ...   \n",
       "18  20  Altman invests in technology ~_~companies and ...   \n",
       "19  21  During the depositor run on Silicon Valley Ban...   \n",
       "20  22  The Lord of the Rings is an epic high-fantasy ...   \n",
       "21  23  Tolkien's ~_~story, after an initially mixed r...   \n",
       "22  24  Ontario is one of the thirteen provinces and t...   \n",
       "23  25  The Iliad is one of two major ancient Greek ep...   \n",
       "24  26  Pink Floyd are an English rock band formed in ...   \n",
       "25  27  The Border Collie is a British breed of herdin...   \n",
       "26  28  The border collie is descended from landrace c...   \n",
       "27  29  South Africa, officially the Republic of South...   \n",
       "28  30  Coldplay are a British rock band formed in Lon...   \n",
       "\n",
       "                                         Output  \n",
       "0                                     developed  \n",
       "1   supernatural,religions,benevolent,religions  \n",
       "2                        emotional,interactions  \n",
       "3             agency,convicts,convicts,convicts  \n",
       "4                                 boasts,bought  \n",
       "5                                     conceived  \n",
       "6                 policy,policy,magazine,policy  \n",
       "7                       series,series,developed  \n",
       "8                           territory,residents  \n",
       "9                                     song,song  \n",
       "10                                    tomb,tomb  \n",
       "11                constructed,monument,monument  \n",
       "12                                   envisioned  \n",
       "13                                     novelist  \n",
       "14                                     writings  \n",
       "15                                      develop  \n",
       "16                                      founded  \n",
       "17                                  corporation  \n",
       "18                                     startups  \n",
       "19                                     startups  \n",
       "20                                        novel  \n",
       "21                                         work  \n",
       "22                            populous,populous  \n",
       "23                        literature,celebrated  \n",
       "24                   groups,successful,produced  \n",
       "25                                         type  \n",
       "26                                        breed  \n",
       "27                                       nation  \n",
       "28                    received,group,repertoire  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training = pd.read_csv(\"./data/wiki_examples_flagged.csv\")\n",
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'syntax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X46sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m df_training \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m./data/wiki_examples_flagged.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m tok, tok_outputs, mask, prob \u001b[39m=\u001b[39m preprocessed(df_training, tokenizer)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X46sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m embd \u001b[39m=\u001b[39m syntax(tok[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][:\u001b[39m1\u001b[39m, :])[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmodel output: \u001b[39m\u001b[39m\"\u001b[39m,embd\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X46sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Instantiate the attention module with the given configuration\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'syntax' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input must now be of shape (batch_size, num_tokens, num_features)\n",
    "# Let's assume a batch_size of 10 for this example\n",
    "batch_size = 2\n",
    "num_tokens = 9\n",
    "input_features = 768\n",
    "\n",
    "config = modules.attn_config(embed_dim=768, \n",
    "                             num_heads=[2, 2], \n",
    "                             dropout=[0.1, 0.1], \n",
    "                             input_dim=input_features, \n",
    "                             dict_dim=30522, \n",
    "                             synonym_head=\"softmax\", \n",
    "                             replace_head=\"sigmoid\")\n",
    "\n",
    "# Generating random input to simulate a batch of sequences\n",
    "df_training = pd.read_csv(\"./data/wiki_examples_flagged.csv\")\n",
    "tok, tok_outputs, mask, prob = preprocessed(df_training, tokenizer)\n",
    "embd = syntax(tok[\"input_ids\"][:1, :])[1]\n",
    "print(\"model output: \",embd.shape)\n",
    "\n",
    "\n",
    "# Instantiate the attention module with the given configuration\n",
    "attn_mech = modules.attn_module(config)\n",
    "\n",
    "# Forward pass through the attention mechanism\n",
    "# Note that config is no longer passed as an argument to the forward method\n",
    "replace_probs, synonym_probs = attn_mech(embd)\n",
    "\n",
    "# Print out shapes and values\n",
    "print(f\"Replacement Probabilities Shape: {replace_probs.shape}\")  # Expected: (batch_size, num_tokens, 1)\n",
    "print(f\"Synonym Probabilities Shape: {synonym_probs.shape}\")      # Expected: (batch_size, num_tokens, dict_dim)\n",
    "# print(f\"Replacement probabilities for each token in Batch 2: {replace_probs[1]}\")           # Expected: A vector of probabilities for each token in the batch. There are three words, so three probabilities.\n",
    "print(f\"Replacement probability Batch 1, Token 1: {replace_probs[0][0]}\")        # Expected: Value between 0 and 1\n",
    "print(f\"Replacement probability Batch 1, Token 1: {replace_probs[0]}\")        # Expected: Value between 0 and 1\n",
    "print(f\"Synonym Probability Distribution for the Batch 1, Token 2: {synonym_probs[0][1]}\") # Expected: A vector of probabilities for each word in the dictionary. There are three words, so three probabilities.\n",
    "print(f\"Synonym Probabilities Sum-to-1 Constraint for Token 1: {torch.sum(synonym_probs[0][0])}\") # Expected : Sum to 1 constraint for the softmax probabilities, for the first token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SyntaxBert'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model        : nn.SyntaxBert, \n",
    "             head         : modules.attn_module,\n",
    "             X            : torch.Tensor,\n",
    "             replacements : torch.Tensor,\n",
    "             synonyms     : torch.Tensor,\n",
    "             optimizer    : any,\n",
    "             loss_fn      : any,\n",
    "             batch_size   : int=16,\n",
    "             epoch        : int=2):\n",
    "    \n",
    "    \n",
    "    # pre-train process =========================================\n",
    "    flag = 10 if epoch > 50 else 1\n",
    "    model_name = type(model).__name__\n",
    "    total_dataset = len(X)\n",
    "    # off load forward and back propagation to the cuda kernel\n",
    "    device = (\n",
    "                \"cuda\"\n",
    "                if torch.cuda.is_available()\n",
    "                else \"mps\"\n",
    "                if torch.backends.mps.is_available()\n",
    "                else \"cpu\"\n",
    "             )\n",
    "\n",
    "    attn_mech.to(device)\n",
    "\n",
    "    # freeze Bert Weights\n",
    "    # \n",
    "    for param in model.parameters():\n",
    "        param.required_grad = False\n",
    "\n",
    "\n",
    "    avg_loss = []\n",
    "    # train process ============================================\n",
    "    for i in range(epoch):\n",
    "        losses = []\n",
    "        for batch in range(0, total_dataset, batch_size):\n",
    "        \n",
    "            x = X[batch:batch+batch_size, ...]\n",
    "            \n",
    "            syn_y = synonyms[batch:batch+batch_size, ...].float()\n",
    "            rep_y = replacements[batch:batch+batch_size, ...].float()\n",
    "            \n",
    "            # for each batch zero grad \n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            _, hidden_layer = model(x)[1]\n",
    "            logits_r, logits_s = head(hidden_layer.to(device))\n",
    "        \n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(logits_s, logits_r, syn_y, rep_y)\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss.append(sum(losses)/len(losses)) \n",
    "\n",
    "        if i % flag == 0:\n",
    "            print(f\"[INFO] |{f'model: {model_name:<5}':^10}|{f'epoch: {i:<5}':^10}|{f'avg loss: {avg_loss[i]:<5}':^10}|\")\n",
    "        \n",
    "    # ==========================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_training = pd.read_csv(\"./data/wiki_examples_flagged.csv\")\n",
    "df_training\n",
    "tokenizer = load_tokenizer()\n",
    "inputs, outputs, replacement, syn = preprocessed(df_training, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([29, 512]),\n",
       " torch.Size([29, 512]),\n",
       " torch.Size([29, 512]),\n",
       " torch.Size([29, 512, 30522]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"].shape, outputs[\"input_ids\"].shape, replacement.shape, syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "syntax = nn.SyntaxBert.load_local_weights(nn.BertConfig)\n",
    "tokenizer = load_tokenizer()\n",
    "df_training = pd.read_csv(\"./wiki_examples_flagged.csv\")\n",
    "\n",
    "\n",
    "lr=0.001\n",
    "optimizer=Adam(syntax.parameters(), lr=lr)\n",
    "loss_fn = loss_module.JointCrossEntropy(head_type=\"linear\")\n",
    "\n",
    "inputs, labels, replacement, synonyms = preprocessed(df_training, tokenizer)\n",
    "\n",
    "config = modules.attn_config(embed_dim=768, \n",
    "                             num_heads=[2, 2], \n",
    "                             dropout=[0.1, 0.1], \n",
    "                             input_dim=input_features, \n",
    "                             dict_dim=30522, \n",
    "                             synonym_head=\"softmax\", \n",
    "                             replace_head=\"sigmoid\")\n",
    "attn_mech = modules.attn_module(config)\n",
    "print\n",
    "training(model=syntax, \n",
    "         head=attn_mech, \n",
    "         X=inputs, \n",
    "         replacements=replacement, \n",
    "         synonyms=synonyms, \n",
    "         optimizer=optimizer, \n",
    "         loss_fn=loss_fn, \n",
    "         batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synonym_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m loss_module\u001b[39m.\u001b[39mJointCrossEntropy(head_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m synonym_probs\u001b[39m.\u001b[39mshape, replace_probs[\u001b[39m0\u001b[39m, :, :]\u001b[39m.\u001b[39mshape, prob[:\u001b[39m1\u001b[39m, :, :]\u001b[39m.\u001b[39mshape, mask[:\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mshape\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCEWithLogitsLoss(reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)(replace_probs[\u001b[39m0\u001b[39m, :, :], mask[:\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'synonym_probs' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = loss_module.JointCrossEntropy(head_type=\"linear\")\n",
    "synonym_probs.shape, replace_probs[0, :, :].shape, prob[:1, :, :].shape, mask[:1, :].T.shape\n",
    "torch.nn.BCEWithLogitsLoss(reduction='mean')(replace_probs[0, :, :], mask[:1, :].T.float())\n",
    "mask.unsqueeze(-1)\n",
    "loss = loss_fn(synonym_probs, replace_probs[0, :, :], prob[:1, :, :].float(), mask[:1, :].T.float())\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
