{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from src.backend.module import nn, modules, loss as loss_module\n",
    "from src.backend.utils import load_base_model, load_tokenizer\n",
    "from torch.optim import Adam\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def ASSERTION_DATAFRAME_COLUMN(columns):\n",
    "    columns = tuple(columns)\n",
    "    DATASET_COLUMNS_WITH_ID = (\"ID\", \"Input\", \"Output\")\n",
    "    DATASET_COLUMNS = (\"Input\", \"Output\")\n",
    "    return DATASET_COLUMNS == columns or DATASET_COLUMNS_WITH_ID == columns\n",
    "\n",
    "\n",
    "def preprocessed(df, tokenizer, vocab=30522, context_length=512, flag=\"~_~\"):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    assert isinstance(df, pd.DataFrame), \"pre-process precondition needs to be in a DataFrame\"\n",
    "    assert ASSERTION_DATAFRAME_COLUMN(df.columns)\n",
    "\n",
    "    # parse string to convert into a list\n",
    "    df[\"Output\"] = df[\"Output\"].apply(lambda x : x.split(','))\n",
    "\n",
    "    # regular expression that obtains flag and associated word\n",
    "    pattern = rf\"{flag}(\\w+)\"\n",
    "\n",
    "    # if ID exist within the column then remove it\n",
    "    if \"ID\" in df.columns:\n",
    "        df.drop(\"ID\", axis=1, inplace=True)\n",
    "\n",
    "    inputs, outputs = [], []\n",
    "    for i, sentence in enumerate(df[\"Input\"]):\n",
    "        replace = re.findall(pattern, sentence) # find all replaced tokens\n",
    "        replacement : list = df[\"Output\"][i]    # index the associated replacement list\n",
    "        \n",
    "        # replace tokens should equal replacement\n",
    "        if len(replacement) != len(replace):\n",
    "            continue\n",
    "\n",
    "        # construct the replacement dictionary\n",
    "        replacement_dict = {replace : replacement[i] for i, replace in enumerate(replace)}   \n",
    "\n",
    "        # TODO Not ideal in initializing each iteration \n",
    "        def replaced(match):\n",
    "            word = match.group(1)\n",
    "            return replacement_dict.get(word, match.group(0))\n",
    "\n",
    "        output_text = re.sub(pattern, replaced, sentence)\n",
    "        input_text = sentence.replace(flag, \"\")\n",
    "        inputs.append(input_text)\n",
    "        outputs.append(output_text)\n",
    "\n",
    "\n",
    "    input_tokens = tokenizer(inputs, padding=True, max_length=context_length, truncation=True, return_tensors=\"pt\")\n",
    "    # TODO: change to make \n",
    "    output_tokens = tokenizer(outputs, padding=True, max_length=context_length, truncation=True, return_tensors=\"pt\")\n",
    "    mask = ((input_tokens[\"input_ids\"] - output_tokens[\"input_ids\"]) != 0).to(torch.long)\n",
    "    prob = torch.nn.functional.one_hot(output_tokens[\"input_ids\"], num_classes=vocab)\n",
    "    \n",
    "    return input_tokens, output_tokens, mask, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/syntax/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "syntax = nn.SyntaxBert.load_local_weights(nn.BertConfig)\n",
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Input</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Labrador Retriever, or simply Labrador, is...</td>\n",
       "      <td>developed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>An angel is a ~_~celestial being in various tr...</td>\n",
       "      <td>supernatural,religions,benevolent,religions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Attachment theory is a psychological, evolutio...</td>\n",
       "      <td>emotional,interactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Gulag was the government ~_~administration...</td>\n",
       "      <td>agency,convicts,convicts,convicts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>TreeHugger is a sustainability website that re...</td>\n",
       "      <td>boasts,bought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Mr. Clean (or Mr. Proper) is a brand name and ...</td>\n",
       "      <td>conceived</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Foreign Affairs is an American journal of inte...</td>\n",
       "      <td>policy,policy,magazine,policy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Power Rangers is an American entertainment and...</td>\n",
       "      <td>series,series,developed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Greenland (Greenlandic: Kalaallit Nunaat, pron...</td>\n",
       "      <td>territory,residents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Amazing Grace' is a Christian hymn published i...</td>\n",
       "      <td>song,song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>The Taj Mahal is an ivory-white marble mausole...</td>\n",
       "      <td>tomb,tomb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>The Eiffel Tower (French: Tour Eiffel) is a wr...</td>\n",
       "      <td>constructed,monument,monument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>The design of the Eiffel Tower is attributed t...</td>\n",
       "      <td>envisioned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>Fyodor Mikhailovich Dostoevsky was a Russian ~...</td>\n",
       "      <td>novelist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>Many of his works are considered highly influe...</td>\n",
       "      <td>writings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>Bose Corporation is an American manufacturing ...</td>\n",
       "      <td>develop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>The company was ~_~established in Massachusett...</td>\n",
       "      <td>founded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>Bose Corporation is an American manufacturing ...</td>\n",
       "      <td>corporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>Altman invests in technology ~_~companies and ...</td>\n",
       "      <td>startups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>During the depositor run on Silicon Valley Ban...</td>\n",
       "      <td>startups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22</td>\n",
       "      <td>The Lord of the Rings is an epic high-fantasy ...</td>\n",
       "      <td>novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>Tolkien's ~_~story, after an initially mixed r...</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>Ontario (on- TAIR -ee-oh; French: […îÃÉta Åjo]...</td>\n",
       "      <td>populous,populous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>The Iliad is one of two major ancient Greek ep...</td>\n",
       "      <td>literature,celebrated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26</td>\n",
       "      <td>Pink Floyd are an English rock band formed in ...</td>\n",
       "      <td>groups,successful,produced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27</td>\n",
       "      <td>The Border Collie is a British breed of herdin...</td>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28</td>\n",
       "      <td>The border collie is descended from landrace c...</td>\n",
       "      <td>breed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>South Africa, officially the Republic of South...</td>\n",
       "      <td>nation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>Coldplay are a British rock band formed in Lon...</td>\n",
       "      <td>received,group,repertoire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                              Input  \\\n",
       "0    1  The Labrador Retriever, or simply Labrador, is...   \n",
       "1    2  An angel is a ~_~celestial being in various tr...   \n",
       "2    3  Attachment theory is a psychological, evolutio...   \n",
       "3    4  The Gulag was the government ~_~administration...   \n",
       "4    5  TreeHugger is a sustainability website that re...   \n",
       "5    6  Mr. Clean (or Mr. Proper) is a brand name and ...   \n",
       "6    7  Foreign Affairs is an American journal of inte...   \n",
       "7    8  Power Rangers is an American entertainment and...   \n",
       "8    9  Greenland (Greenlandic: Kalaallit Nunaat, pron...   \n",
       "9   10  Amazing Grace' is a Christian hymn published i...   \n",
       "10  11  The Taj Mahal is an ivory-white marble mausole...   \n",
       "11  13  The Eiffel Tower (French: Tour Eiffel) is a wr...   \n",
       "12  14  The design of the Eiffel Tower is attributed t...   \n",
       "13  15  Fyodor Mikhailovich Dostoevsky was a Russian ~...   \n",
       "14  16  Many of his works are considered highly influe...   \n",
       "15  17  Bose Corporation is an American manufacturing ...   \n",
       "16  18  The company was ~_~established in Massachusett...   \n",
       "17  19  Bose Corporation is an American manufacturing ...   \n",
       "18  20  Altman invests in technology ~_~companies and ...   \n",
       "19  21  During the depositor run on Silicon Valley Ban...   \n",
       "20  22  The Lord of the Rings is an epic high-fantasy ...   \n",
       "21  23  Tolkien's ~_~story, after an initially mixed r...   \n",
       "22  24  Ontario (on- TAIR -ee-oh; French: […îÃÉta Åjo]...   \n",
       "23  25  The Iliad is one of two major ancient Greek ep...   \n",
       "24  26  Pink Floyd are an English rock band formed in ...   \n",
       "25  27  The Border Collie is a British breed of herdin...   \n",
       "26  28  The border collie is descended from landrace c...   \n",
       "27  29  South Africa, officially the Republic of South...   \n",
       "28  30  Coldplay are a British rock band formed in Lon...   \n",
       "\n",
       "                                         Output  \n",
       "0                                     developed  \n",
       "1   supernatural,religions,benevolent,religions  \n",
       "2                        emotional,interactions  \n",
       "3             agency,convicts,convicts,convicts  \n",
       "4                                 boasts,bought  \n",
       "5                                     conceived  \n",
       "6                 policy,policy,magazine,policy  \n",
       "7                       series,series,developed  \n",
       "8                           territory,residents  \n",
       "9                                     song,song  \n",
       "10                                    tomb,tomb  \n",
       "11                constructed,monument,monument  \n",
       "12                                   envisioned  \n",
       "13                                     novelist  \n",
       "14                                     writings  \n",
       "15                                      develop  \n",
       "16                                      founded  \n",
       "17                                  corporation  \n",
       "18                                     startups  \n",
       "19                                     startups  \n",
       "20                                        novel  \n",
       "21                                         work  \n",
       "22                            populous,populous  \n",
       "23                        literature,celebrated  \n",
       "24                   groups,successful,produced  \n",
       "25                                         type  \n",
       "26                                        breed  \n",
       "27                                       nation  \n",
       "28                    received,group,repertoire  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training = pd.read_csv(\"./data/wiki_examples_flagged.csv\")\n",
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output:  torch.Size([1, 512, 768])\n",
      "Replacement Probabilities Shape: torch.Size([1, 512, 1])\n",
      "Synonym Probabilities Shape: torch.Size([1, 512, 30522])\n",
      "Replacement probability Batch 1, Token 1: tensor([0.4955], grad_fn=<SelectBackward0>)\n",
      "Replacement probability Batch 1, Token 1: tensor([[0.4955],\n",
      "        [0.4960],\n",
      "        [0.4957],\n",
      "        [0.4958],\n",
      "        [0.4956],\n",
      "        [0.4952],\n",
      "        [0.4957],\n",
      "        [0.4955],\n",
      "        [0.4956],\n",
      "        [0.4957],\n",
      "        [0.4958],\n",
      "        [0.4957],\n",
      "        [0.4959],\n",
      "        [0.4962],\n",
      "        [0.4959],\n",
      "        [0.4959],\n",
      "        [0.4958],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4959],\n",
      "        [0.4952],\n",
      "        [0.4959],\n",
      "        [0.4963],\n",
      "        [0.4955],\n",
      "        [0.4956],\n",
      "        [0.4954],\n",
      "        [0.4958],\n",
      "        [0.4961],\n",
      "        [0.4959],\n",
      "        [0.4954],\n",
      "        [0.4956],\n",
      "        [0.4955],\n",
      "        [0.4964],\n",
      "        [0.4963],\n",
      "        [0.4958],\n",
      "        [0.4961],\n",
      "        [0.4954],\n",
      "        [0.4957],\n",
      "        [0.4961],\n",
      "        [0.4957],\n",
      "        [0.4959],\n",
      "        [0.4955],\n",
      "        [0.4956],\n",
      "        [0.4956],\n",
      "        [0.4956],\n",
      "        [0.4957],\n",
      "        [0.4958],\n",
      "        [0.4957],\n",
      "        [0.4955],\n",
      "        [0.4958],\n",
      "        [0.4956],\n",
      "        [0.4958],\n",
      "        [0.4953],\n",
      "        [0.4964],\n",
      "        [0.4957],\n",
      "        [0.4958],\n",
      "        [0.4956],\n",
      "        [0.4958],\n",
      "        [0.4956],\n",
      "        [0.4961],\n",
      "        [0.4960],\n",
      "        [0.4957],\n",
      "        [0.4955],\n",
      "        [0.4956],\n",
      "        [0.4956],\n",
      "        [0.4953],\n",
      "        [0.4951],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4955],\n",
      "        [0.4965],\n",
      "        [0.4953],\n",
      "        [0.4962],\n",
      "        [0.4955],\n",
      "        [0.4959],\n",
      "        [0.4959],\n",
      "        [0.4953],\n",
      "        [0.4956],\n",
      "        [0.4952],\n",
      "        [0.4950],\n",
      "        [0.4956],\n",
      "        [0.4955],\n",
      "        [0.4952],\n",
      "        [0.4956],\n",
      "        [0.4960],\n",
      "        [0.4955],\n",
      "        [0.4952],\n",
      "        [0.4960],\n",
      "        [0.4959],\n",
      "        [0.4959],\n",
      "        [0.4959],\n",
      "        [0.4962],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4960],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4961],\n",
      "        [0.4952],\n",
      "        [0.4958],\n",
      "        [0.4958],\n",
      "        [0.4952],\n",
      "        [0.4958],\n",
      "        [0.4955],\n",
      "        [0.4954],\n",
      "        [0.4961],\n",
      "        [0.4953],\n",
      "        [0.4960],\n",
      "        [0.4957],\n",
      "        [0.4960],\n",
      "        [0.4956],\n",
      "        [0.4964],\n",
      "        [0.4957],\n",
      "        [0.4958],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4956],\n",
      "        [0.4953],\n",
      "        [0.4957],\n",
      "        [0.4959],\n",
      "        [0.4955],\n",
      "        [0.4958],\n",
      "        [0.4955],\n",
      "        [0.4954],\n",
      "        [0.4951],\n",
      "        [0.4957],\n",
      "        [0.4957],\n",
      "        [0.4956],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4954],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4956],\n",
      "        [0.4953],\n",
      "        [0.4957],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4954],\n",
      "        [0.4949],\n",
      "        [0.4955],\n",
      "        [0.4954],\n",
      "        [0.4955],\n",
      "        [0.4950],\n",
      "        [0.4955],\n",
      "        [0.4954],\n",
      "        [0.4951],\n",
      "        [0.4952],\n",
      "        [0.4954],\n",
      "        [0.4952],\n",
      "        [0.4954],\n",
      "        [0.4952],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4957],\n",
      "        [0.4956],\n",
      "        [0.4959],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4957],\n",
      "        [0.4958],\n",
      "        [0.4950],\n",
      "        [0.4949],\n",
      "        [0.4956],\n",
      "        [0.4953],\n",
      "        [0.4956],\n",
      "        [0.4950],\n",
      "        [0.4956],\n",
      "        [0.4955],\n",
      "        [0.4952],\n",
      "        [0.4954],\n",
      "        [0.4956],\n",
      "        [0.4949],\n",
      "        [0.4953],\n",
      "        [0.4959],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4954],\n",
      "        [0.4958],\n",
      "        [0.4955],\n",
      "        [0.4951],\n",
      "        [0.4961],\n",
      "        [0.4955],\n",
      "        [0.4959],\n",
      "        [0.4955],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4952],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4952],\n",
      "        [0.4957],\n",
      "        [0.4957],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4954],\n",
      "        [0.4952],\n",
      "        [0.4954],\n",
      "        [0.4953],\n",
      "        [0.4950],\n",
      "        [0.4956],\n",
      "        [0.4952],\n",
      "        [0.4956],\n",
      "        [0.4955],\n",
      "        [0.4958],\n",
      "        [0.4955],\n",
      "        [0.4957],\n",
      "        [0.4956],\n",
      "        [0.4957],\n",
      "        [0.4958],\n",
      "        [0.4952],\n",
      "        [0.4954],\n",
      "        [0.4959],\n",
      "        [0.4958],\n",
      "        [0.4951],\n",
      "        [0.4949],\n",
      "        [0.4951],\n",
      "        [0.4953],\n",
      "        [0.4959],\n",
      "        [0.4957],\n",
      "        [0.4952],\n",
      "        [0.4952],\n",
      "        [0.4955],\n",
      "        [0.4956],\n",
      "        [0.4955],\n",
      "        [0.4956],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4951],\n",
      "        [0.4958],\n",
      "        [0.4954],\n",
      "        [0.4957],\n",
      "        [0.4952],\n",
      "        [0.4956],\n",
      "        [0.4955],\n",
      "        [0.4956],\n",
      "        [0.4957],\n",
      "        [0.4953],\n",
      "        [0.4954],\n",
      "        [0.4953],\n",
      "        [0.4959],\n",
      "        [0.4953],\n",
      "        [0.4952],\n",
      "        [0.4954],\n",
      "        [0.4955],\n",
      "        [0.4952],\n",
      "        [0.4957],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4952],\n",
      "        [0.4951],\n",
      "        [0.4951],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4958],\n",
      "        [0.4958],\n",
      "        [0.4957],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4951],\n",
      "        [0.4957],\n",
      "        [0.4955],\n",
      "        [0.4954],\n",
      "        [0.4950],\n",
      "        [0.4959],\n",
      "        [0.4954],\n",
      "        [0.4956],\n",
      "        [0.4955],\n",
      "        [0.4957],\n",
      "        [0.4950],\n",
      "        [0.4956],\n",
      "        [0.4954],\n",
      "        [0.4952],\n",
      "        [0.4949],\n",
      "        [0.4951],\n",
      "        [0.4949],\n",
      "        [0.4951],\n",
      "        [0.4952],\n",
      "        [0.4953],\n",
      "        [0.4959],\n",
      "        [0.4951],\n",
      "        [0.4952],\n",
      "        [0.4955],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4952],\n",
      "        [0.4955],\n",
      "        [0.4954],\n",
      "        [0.4955],\n",
      "        [0.4950],\n",
      "        [0.4957],\n",
      "        [0.4950],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4953],\n",
      "        [0.4960],\n",
      "        [0.4957],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4950],\n",
      "        [0.4952],\n",
      "        [0.4953],\n",
      "        [0.4954],\n",
      "        [0.4957],\n",
      "        [0.4950],\n",
      "        [0.4951],\n",
      "        [0.4957],\n",
      "        [0.4957],\n",
      "        [0.4955],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4956],\n",
      "        [0.4951],\n",
      "        [0.4954],\n",
      "        [0.4952],\n",
      "        [0.4950],\n",
      "        [0.4960],\n",
      "        [0.4960],\n",
      "        [0.4954],\n",
      "        [0.4957],\n",
      "        [0.4953],\n",
      "        [0.4950],\n",
      "        [0.4953],\n",
      "        [0.4956],\n",
      "        [0.4962],\n",
      "        [0.4957],\n",
      "        [0.4955],\n",
      "        [0.4957],\n",
      "        [0.4955],\n",
      "        [0.4956],\n",
      "        [0.4954],\n",
      "        [0.4952],\n",
      "        [0.4956],\n",
      "        [0.4958],\n",
      "        [0.4955],\n",
      "        [0.4951],\n",
      "        [0.4956],\n",
      "        [0.4951],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4951],\n",
      "        [0.4952],\n",
      "        [0.4953],\n",
      "        [0.4950],\n",
      "        [0.4954],\n",
      "        [0.4957],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4950],\n",
      "        [0.4954],\n",
      "        [0.4953],\n",
      "        [0.4950],\n",
      "        [0.4952],\n",
      "        [0.4951],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4958],\n",
      "        [0.4954],\n",
      "        [0.4953],\n",
      "        [0.4952],\n",
      "        [0.4954],\n",
      "        [0.4956],\n",
      "        [0.4960],\n",
      "        [0.4953],\n",
      "        [0.4958],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4950],\n",
      "        [0.4952],\n",
      "        [0.4957],\n",
      "        [0.4947],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4954],\n",
      "        [0.4956],\n",
      "        [0.4959],\n",
      "        [0.4951],\n",
      "        [0.4954],\n",
      "        [0.4957],\n",
      "        [0.4954],\n",
      "        [0.4956],\n",
      "        [0.4957],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4957],\n",
      "        [0.4950],\n",
      "        [0.4944],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4954],\n",
      "        [0.4952],\n",
      "        [0.4953],\n",
      "        [0.4950],\n",
      "        [0.4952],\n",
      "        [0.4953],\n",
      "        [0.4952],\n",
      "        [0.4949],\n",
      "        [0.4957],\n",
      "        [0.4956],\n",
      "        [0.4952],\n",
      "        [0.4949],\n",
      "        [0.4951],\n",
      "        [0.4958],\n",
      "        [0.4956],\n",
      "        [0.4957],\n",
      "        [0.4959],\n",
      "        [0.4948],\n",
      "        [0.4952],\n",
      "        [0.4959],\n",
      "        [0.4957],\n",
      "        [0.4955],\n",
      "        [0.4954],\n",
      "        [0.4958],\n",
      "        [0.4952],\n",
      "        [0.4956],\n",
      "        [0.4953],\n",
      "        [0.4951],\n",
      "        [0.4953],\n",
      "        [0.4951],\n",
      "        [0.4954],\n",
      "        [0.4958],\n",
      "        [0.4956],\n",
      "        [0.4956],\n",
      "        [0.4948],\n",
      "        [0.4958],\n",
      "        [0.4952],\n",
      "        [0.4955],\n",
      "        [0.4951],\n",
      "        [0.4948],\n",
      "        [0.4960],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4951],\n",
      "        [0.4953],\n",
      "        [0.4948],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4954],\n",
      "        [0.4959],\n",
      "        [0.4952],\n",
      "        [0.4956],\n",
      "        [0.4952],\n",
      "        [0.4955],\n",
      "        [0.4955],\n",
      "        [0.4957],\n",
      "        [0.4954],\n",
      "        [0.4951],\n",
      "        [0.4956],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4958],\n",
      "        [0.4960],\n",
      "        [0.4954],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4954],\n",
      "        [0.4960],\n",
      "        [0.4957],\n",
      "        [0.4950],\n",
      "        [0.4962],\n",
      "        [0.4956],\n",
      "        [0.4954],\n",
      "        [0.4957],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4953],\n",
      "        [0.4956],\n",
      "        [0.4953],\n",
      "        [0.4953],\n",
      "        [0.4955],\n",
      "        [0.4955],\n",
      "        [0.4957],\n",
      "        [0.4958],\n",
      "        [0.4953],\n",
      "        [0.4956],\n",
      "        [0.4958],\n",
      "        [0.4956],\n",
      "        [0.4956],\n",
      "        [0.4954],\n",
      "        [0.4955],\n",
      "        [0.4952],\n",
      "        [0.4955],\n",
      "        [0.4951],\n",
      "        [0.4954],\n",
      "        [0.4956],\n",
      "        [0.4952],\n",
      "        [0.4956],\n",
      "        [0.4952],\n",
      "        [0.4952],\n",
      "        [0.4950],\n",
      "        [0.4954],\n",
      "        [0.4954],\n",
      "        [0.4950],\n",
      "        [0.4957],\n",
      "        [0.4955],\n",
      "        [0.4957],\n",
      "        [0.4956],\n",
      "        [0.4952],\n",
      "        [0.4956],\n",
      "        [0.4953],\n",
      "        [0.4950],\n",
      "        [0.4959],\n",
      "        [0.4953],\n",
      "        [0.4954]], grad_fn=<SelectBackward0>)\n",
      "Synonym Probability Distribution for the Batch 1, Token 2: tensor([3.2057e-05, 3.2989e-05, 3.5144e-05,  ..., 3.2568e-05, 3.2415e-05,\n",
      "        2.9987e-05], grad_fn=<SelectBackward0>)\n",
      "Synonym Probabilities Sum-to-1 Constraint for Token 1: 0.9999998807907104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input must now be of shape (batch_size, num_tokens, num_features)\n",
    "# Let's assume a batch_size of 10 for this example\n",
    "batch_size = 2\n",
    "num_tokens = 9\n",
    "input_features = 768\n",
    "\n",
    "config = modules.attn_config(embed_dim=768, \n",
    "                             num_heads=[2, 2], \n",
    "                             dropout=[0.1, 0.1], \n",
    "                             input_dim=input_features, \n",
    "                             dict_dim=30522, \n",
    "                             synonym_head=\"softmax\", \n",
    "                             replace_head=\"sigmoid\")\n",
    "\n",
    "# Generating random input to simulate a batch of sequences\n",
    "df_training = pd.read_csv(\"./wiki_examples_flagged.csv\")\n",
    "tok, tok_outputs, mask, prob = preprocessed(df_training, tokenizer)\n",
    "embd = syntax(tok[\"input_ids\"][:1, :])[1]\n",
    "print(\"model output: \",embd.shape)\n",
    "\n",
    "\n",
    "# Instantiate the attention module with the given configuration\n",
    "attn_mech = modules.attn_module(config)\n",
    "\n",
    "# Forward pass through the attention mechanism\n",
    "# Note that config is no longer passed as an argument to the forward method\n",
    "replace_probs, synonym_probs = attn_mech(embd)\n",
    "\n",
    "# Print out shapes and values\n",
    "print(f\"Replacement Probabilities Shape: {replace_probs.shape}\")  # Expected: (batch_size, num_tokens, 1)\n",
    "print(f\"Synonym Probabilities Shape: {synonym_probs.shape}\")      # Expected: (batch_size, num_tokens, dict_dim)\n",
    "# print(f\"Replacement probabilities for each token in Batch 2: {replace_probs[1]}\")           # Expected: A vector of probabilities for each token in the batch. There are three words, so three probabilities.\n",
    "print(f\"Replacement probability Batch 1, Token 1: {replace_probs[0][0]}\")        # Expected: Value between 0 and 1\n",
    "print(f\"Replacement probability Batch 1, Token 1: {replace_probs[0]}\")        # Expected: Value between 0 and 1\n",
    "print(f\"Synonym Probability Distribution for the Batch 1, Token 2: {synonym_probs[0][1]}\") # Expected: A vector of probabilities for each word in the dictionary. There are three words, so three probabilities.\n",
    "print(f\"Synonym Probabilities Sum-to-1 Constraint for Token 1: {torch.sum(synonym_probs[0][0])}\") # Expected : Sum to 1 constraint for the softmax probabilities, for the first token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SyntaxBert'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model        : nn.SyntaxBert, \n",
    "             head         : modules.attn_module,\n",
    "             X            : torch.Tensor,\n",
    "             replacements : torch.Tensor,\n",
    "             synonyms     : torch.Tensor,\n",
    "             optimizer    : any,\n",
    "             loss_fn      : any,\n",
    "             batch_size   : int=16,\n",
    "             epoch        : int=2):\n",
    "    \n",
    "    \n",
    "    # pre-train process =========================================\n",
    "    flag = 10 if epoch > 50 else 1\n",
    "    model_name = type(model).__name__\n",
    "    total_dataset = len(X)\n",
    "    # off load forward and back propagation to the cuda kernel\n",
    "    device = (\n",
    "                \"cuda\"\n",
    "                if torch.cuda.is_available()\n",
    "                else \"mps\"\n",
    "                if torch.backends.mps.is_available()\n",
    "                else \"cpu\"\n",
    "             )\n",
    "\n",
    "    attn_mech.to(device)\n",
    "\n",
    "    # freeze Bert Weights\n",
    "    # \n",
    "    for param in model.parameters():\n",
    "        param.required_grad = False\n",
    "\n",
    "\n",
    "    avg_loss = []\n",
    "    # train process ============================================\n",
    "    for i in range(epoch):\n",
    "        losses = []\n",
    "        for batch in range(0, total_dataset, batch_size):\n",
    "        \n",
    "            x = X[batch:batch+batch_size, ...]\n",
    "            \n",
    "            syn_y = synonyms[batch:batch+batch_size, ...].float()\n",
    "            rep_y = replacements[batch:batch+batch_size, ...].float()\n",
    "            \n",
    "            # for each batch zero grad \n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            _, hidden_layer = model(x)[1]\n",
    "            logits_r, logits_s = head(hidden_layer.to(device))\n",
    "        \n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(logits_s, logits_r, syn_y, rep_y)\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss.append(sum(losses)/len(losses)) \n",
    "\n",
    "        if i % flag == 0:\n",
    "            print(f\"[INFO] |{f'model: {model_name:<5}':^10}|{f'epoch: {i:<5}':^10}|{f'avg loss: {avg_loss[i]:<5}':^10}|\")\n",
    "        \n",
    "    # ==========================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_training = pd.read_csv(\"./data/wiki_examples_flagged.csv\")\n",
    "df_training\n",
    "tokenizer = load_tokenizer()\n",
    "inputs, outputs, replacement, syn = preprocessed(df_training, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([29, 512]),\n",
       " torch.Size([29, 512]),\n",
       " torch.Size([29, 512]),\n",
       " torch.Size([29, 512, 30522]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"].shape, outputs[\"input_ids\"].shape, replacement.shape, syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "syntax = nn.SyntaxBert.load_local_weights(nn.BertConfig)\n",
    "tokenizer = load_tokenizer()\n",
    "df_training = pd.read_csv(\"./wiki_examples_flagged.csv\")\n",
    "\n",
    "\n",
    "lr=0.001\n",
    "optimizer=Adam(syntax.parameters(), lr=lr)\n",
    "loss_fn = loss_module.JointCrossEntropy(head_type=\"linear\")\n",
    "\n",
    "inputs, labels, replacement, synonyms = preprocessed(df_training, tokenizer)\n",
    "\n",
    "config = modules.attn_config(embed_dim=768, \n",
    "                             num_heads=[2, 2], \n",
    "                             dropout=[0.1, 0.1], \n",
    "                             input_dim=input_features, \n",
    "                             dict_dim=30522, \n",
    "                             synonym_head=\"softmax\", \n",
    "                             replace_head=\"sigmoid\")\n",
    "attn_mech = modules.attn_module(config)\n",
    "\n",
    "training(model=syntax, \n",
    "         head=attn_mech, \n",
    "         X=inputs[\"input_ids\"], \n",
    "         replacements=replacement, \n",
    "         synonyms=synonyms, \n",
    "         optimizer=optimizer, \n",
    "         loss_fn=loss_fn, \n",
    "         batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mJointCrossEntropy(head_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m synonym_probs\u001b[39m.\u001b[39mshape, replace_probs[\u001b[39m0\u001b[39m, :, :]\u001b[39m.\u001b[39mshape, prob[:\u001b[39m1\u001b[39m, :, :]\u001b[39m.\u001b[39mshape, mask[:\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mshape\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucavivona/Documents/Programs/York/EECS/4044/Syntax/playground.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCEWithLogitsLoss(reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)(replace_probs[\u001b[39m0\u001b[39m, :, :], mask[:\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mfloat())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = loss.JointCrossEntropy(head_type=\"linear\")\n",
    "synonym_probs.shape, replace_probs[0, :, :].shape, prob[:1, :, :].shape, mask[:1, :].T.shape\n",
    "torch.nn.BCEWithLogitsLoss(reduction='mean')(replace_probs[0, :, :], mask[:1, :].T.float())\n",
    "mask.unsqueeze(-1)\n",
    "loss = loss_fn(synonym_probs, replace_probs[0, :, :], prob[:1, :, :].float(), mask[:1, :].T.float())\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
